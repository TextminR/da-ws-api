%!TEX root=./thesis.tex

% Hier finden sich alle Referenzen aus Literatur und Internet, welche innerhalb der eigentlichen Arbeit verwendet wurden.
% Zur einfachen Anwendung werden hier einige Beispiele gegeben.

% Nutzung einer Quelle, welche über das Internet zugänglich ist.
% Einige Parameter sollten bei jeder Referenz vorhanden sein:
% - title: 			Der Titel der Website (wird meist in der Adresszeile angezeigt)
% - author:			Alle Autoren, welche als für den Text verantwortlich gegeben sind. Sollte keiner angegeben sein, könnte etwa „Autoren von {Seitenname}“ verwendet werden
% - url:			Ein eindeutiger Link zur referenzierten Quelle, welcher im Optimalfall über mehrere Jahre hinweg verfügbar ist
% - note:			Ebenfalls sollte das Datum des letzten Zugriffs in einer Notiz angegeben
@online{ wiki:syt,
	title = {Systemtechnik},
	author = {Wikipedia Autoren},
	url = {https://de.wikipedia.org/wiki/Systemtechnik},
	organization = {Wikipedia},
	note = {07.03.2018}
}

@article{koder2021increasing,
  title={Increasing Full Stack Development Productivity via Technology Selection},
  author={Koder, Mike},
  year={2021}
}

% Nutzung von Inhalten eines Buchs.
% Bei Büchern gibt es einige Angaben, welche auf jeden Fall angegeben werden müssen:
% - title: 		Der Vollständige Titel des Buchs
% - author: 	Alle beteiligten Autoren
% - year:		Erscheinungsdatum der verwendeten Auflage (siehe auch Parameter „edition“)
% - edition:	Aktuelle Auflage des Buchs (wenn nicht angegeben: 1. Auflage verwenden) und Markierung spezieller Versionen
% - publisher:	Der Verlag, welcher das Buch herausgab (häufig werden vom Verlag noch kleiner Änderungen vorgenommen)
% - pages:		Alle Seiten, durch Komma getrennt, welche innerhalb der Arbeit verwendet wurden (mehrere Seiten können auch mit „-“ angegeben werden)
%
% Optional sind auch folgende Parameter interessant:
% - subtitle:	Falls Bücher sich etwa nur durch ihren Untertitel unterscheiden
% - pagetotal:	Gesamte Anzahl der Seiten der verwendeten Quelle
@book{ physik1,
	title = {Physik 1},
	author = {Schweitzer, Christian and Svoboda, Peter and Trieb, Lutz},
	year = {2011},
	subtitle = {Mechanik, Thermodynamik, Optik},
	edition = {7. Auflage},
	publisher = {Veritas},
	pages = {140, 145-150},
	pagetotal = {296}
}

%alex
@article{BABU2020,
   abstract = {Web scraping is basically an interactive method for website and some other online sources to browse for and access data. To delete a replica of the information and save it in an external archive for review, it uses software engineering technology and custom software programming to extract data or any other content of on-line sources. Web scraping is often called automatic data gathering, database discovery, database crawling, or content management mining. Web scraping have possibly existed since before the start of the World Wide Web, but it has been used mainly in the context of data analytics, and is generally associated to e-commerce. Web scraping technique provides a broad collection of options and can serve various purposes: A web crawler's least necessity is to automate the normally physical work of gathering cost quotation marks and website article details. A web crawler's main requirement will be to discover formerly inaccessible sources of price data, and include a survey of all accessible price information. This scraping process is performed using different technologies which can be automatic application tools or manual methods. This paper provides the overall review of web scraping technology, how it is carried out and the effects of this technology.},
   author = {KIRAN BABU},
   doi = {10.37896/whjj16.06/001},
   issn = {10011749},
   issue = {06},
   journal = {WAFFEN-UND KOSTUMKUNDE JOURNAL},
   title = {Survey on Web scraping technology},
   volume = {16},
   year = {2020},
}
@article{Foerderer2023,
   abstract = {The increasing adoption of econometric and machine-learning approaches by empirical researchers has led to a widespread use of one data collection method: web scraping. Web scraping refers to the use of automated computer programs to access websites and download their content. The key argument of this paper is that na\"ive web scraping procedures can lead to sampling bias in the collected data. This article describes three sources of sampling bias in web-scraped data. More specifically, sampling bias emerges from web content being volatile (i.e., being subject to change), personalized (i.e., presented in response to request characteristics), and unindexed (i.e., abundance of a population register). In a series of examples, I illustrate the prevalence and magnitude of sampling bias. To support researchers and reviewers, this paper provides recommendations on anticipating, detecting, and overcoming sampling bias in web-scraped data.},
   author = {Jens Foerderer},
   month = {8},
   title = {Should we trust web-scraped data?},
   url = {http://arxiv.org/abs/2308.02231},
   year = {2023},
}

